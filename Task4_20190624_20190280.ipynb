{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oUFaoVoHDYR",
        "outputId": "39a4a0fb-febd-413a-d4b0-48f4ea5daeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense,SimpleRNN,LSTM, GRU\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "hSs0KEvYtj2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Data**"
      ],
      "metadata": {
        "id": "uakwZAEB8Lo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/dataset/weather_prediction_dataset.csv')\n"
      ],
      "metadata": {
        "id": "ub4dsSqfH-Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4eXWhSIwKkjV",
        "outputId": "6600a5f5-08e0-45b4-80dc-df68913396ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          DATE  MONTH  BASEL_cloud_cover  BASEL_humidity  BASEL_pressure  \\\n",
              "0     20000101      1                  8            0.89          1.0286   \n",
              "1     20000102      1                  8            0.87          1.0318   \n",
              "2     20000103      1                  5            0.81          1.0314   \n",
              "3     20000104      1                  7            0.79          1.0262   \n",
              "4     20000105      1                  5            0.90          1.0246   \n",
              "...        ...    ...                ...             ...             ...   \n",
              "3649  20091228     12                  7            0.82          1.0084   \n",
              "3650  20091229     12                  7            0.92          1.0028   \n",
              "3651  20091230     12                  8            0.92          0.9979   \n",
              "3652  20091231     12                  7            0.93          0.9958   \n",
              "3653  20100101      1                  8            0.93          0.9965   \n",
              "\n",
              "      BASEL_global_radiation  BASEL_precipitation  BASEL_sunshine  \\\n",
              "0                       0.20                 0.03             0.0   \n",
              "1                       0.25                 0.00             0.0   \n",
              "2                       0.50                 0.00             3.7   \n",
              "3                       0.63                 0.35             6.9   \n",
              "4                       0.51                 0.07             3.7   \n",
              "...                      ...                  ...             ...   \n",
              "3649                    0.28                 0.42             0.3   \n",
              "3650                    0.22                 1.68             0.2   \n",
              "3651                    0.07                 1.54             0.0   \n",
              "3652                    0.17                 0.57             0.1   \n",
              "3653                    0.08                 0.56             0.0   \n",
              "\n",
              "      BASEL_temp_mean  BASEL_temp_min  ...  STOCKHOLM_temp_min  \\\n",
              "0                 2.9             1.6  ...                -9.3   \n",
              "1                 3.6             2.7  ...                 0.5   \n",
              "2                 2.2             0.1  ...                -1.0   \n",
              "3                 3.9             0.5  ...                 2.5   \n",
              "4                 6.0             3.8  ...                -1.8   \n",
              "...               ...             ...  ...                 ...   \n",
              "3649              3.2             1.0  ...                -2.7   \n",
              "3650              4.5             2.4  ...                -9.5   \n",
              "3651              8.5             7.5  ...               -12.5   \n",
              "3652              6.6             4.3  ...                -9.3   \n",
              "3653              2.9            -0.2  ...                -8.8   \n",
              "\n",
              "      STOCKHOLM_temp_max  TOURS_wind_speed  TOURS_humidity  TOURS_pressure  \\\n",
              "0                    0.7               1.6            0.97          1.0275   \n",
              "1                    2.0               2.0            0.99          1.0293   \n",
              "2                    2.8               3.4            0.91          1.0267   \n",
              "3                    4.6               4.9            0.95          1.0222   \n",
              "4                    2.9               3.6            0.95          1.0209   \n",
              "...                  ...               ...             ...             ...   \n",
              "3649                 2.4               3.7            0.95          1.0011   \n",
              "3650                 0.8               5.3            0.89          0.9966   \n",
              "3651                -7.4               3.8            0.88          0.9939   \n",
              "3652                -6.5               4.2            0.88          0.9933   \n",
              "3653                -7.0               3.4            0.86          1.0040   \n",
              "\n",
              "      TOURS_global_radiation  TOURS_precipitation  TOURS_temp_mean  \\\n",
              "0                       0.25                 0.04              8.5   \n",
              "1                       0.17                 0.16              7.9   \n",
              "2                       0.27                 0.00              8.1   \n",
              "3                       0.11                 0.44              8.6   \n",
              "4                       0.39                 0.04              8.0   \n",
              "...                      ...                  ...              ...   \n",
              "3649                    0.22                 1.50              6.2   \n",
              "3650                    0.24                 0.40             10.4   \n",
              "3651                    0.24                 1.00             10.0   \n",
              "3652                    0.58                 0.02              8.5   \n",
              "3653                    0.11                 0.00              0.5   \n",
              "\n",
              "      TOURS_temp_min  TOURS_temp_max  \n",
              "0                7.2             9.8  \n",
              "1                6.6             9.2  \n",
              "2                6.6             9.6  \n",
              "3                6.4            10.8  \n",
              "4                6.4             9.5  \n",
              "...              ...             ...  \n",
              "3649             1.8            10.6  \n",
              "3650             6.2            14.5  \n",
              "3651             8.7            11.3  \n",
              "3652             6.2            10.9  \n",
              "3653            -0.7             1.8  \n",
              "\n",
              "[3654 rows x 165 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3bdae42-3b8a-4e81-8c7b-d13784f2a524\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>MONTH</th>\n",
              "      <th>BASEL_cloud_cover</th>\n",
              "      <th>BASEL_humidity</th>\n",
              "      <th>BASEL_pressure</th>\n",
              "      <th>BASEL_global_radiation</th>\n",
              "      <th>BASEL_precipitation</th>\n",
              "      <th>BASEL_sunshine</th>\n",
              "      <th>BASEL_temp_mean</th>\n",
              "      <th>BASEL_temp_min</th>\n",
              "      <th>...</th>\n",
              "      <th>STOCKHOLM_temp_min</th>\n",
              "      <th>STOCKHOLM_temp_max</th>\n",
              "      <th>TOURS_wind_speed</th>\n",
              "      <th>TOURS_humidity</th>\n",
              "      <th>TOURS_pressure</th>\n",
              "      <th>TOURS_global_radiation</th>\n",
              "      <th>TOURS_precipitation</th>\n",
              "      <th>TOURS_temp_mean</th>\n",
              "      <th>TOURS_temp_min</th>\n",
              "      <th>TOURS_temp_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20000101</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.89</td>\n",
              "      <td>1.0286</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.6</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1.0275</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7.2</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20000102</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.87</td>\n",
              "      <td>1.0318</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.0293</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.16</td>\n",
              "      <td>7.9</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20000103</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1.0314</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.7</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.91</td>\n",
              "      <td>1.0267</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20000104</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.0262</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.35</td>\n",
              "      <td>6.9</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.9</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.0222</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.44</td>\n",
              "      <td>8.6</td>\n",
              "      <td>6.4</td>\n",
              "      <td>10.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20000105</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.0246</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.07</td>\n",
              "      <td>3.7</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.8</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.0209</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.04</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.4</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3649</th>\n",
              "      <td>20091228</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.0084</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.7</td>\n",
              "      <td>2.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.0011</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.50</td>\n",
              "      <td>6.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3650</th>\n",
              "      <td>20091229</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1.0028</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2.4</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.5</td>\n",
              "      <td>0.8</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.40</td>\n",
              "      <td>10.4</td>\n",
              "      <td>6.2</td>\n",
              "      <td>14.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3651</th>\n",
              "      <td>20091230</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7.5</td>\n",
              "      <td>...</td>\n",
              "      <td>-12.5</td>\n",
              "      <td>-7.4</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9939</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.00</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>11.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3652</th>\n",
              "      <td>20091231</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9958</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>4.3</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>-6.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9933</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.02</td>\n",
              "      <td>8.5</td>\n",
              "      <td>6.2</td>\n",
              "      <td>10.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3653</th>\n",
              "      <td>20100101</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.8</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.0040</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3654 rows × 165 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3bdae42-3b8a-4e81-8c7b-d13784f2a524')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3bdae42-3b8a-4e81-8c7b-d13784f2a524 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3bdae42-3b8a-4e81-8c7b-d13784f2a524');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**"
      ],
      "metadata": {
        "id": "imqql80x8Pgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['BASEL_humidity' , 'BASEL_pressure' ,'BASEL_sunshine','TOURS_temp_mean','TOURS_temp_min', 'TOURS_temp_max']\n",
        "df_1 = df[features]"
      ],
      "metadata": {
        "id": "6oXvSctHQo3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aToQFxJ0RU4V",
        "outputId": "cf8fd7f8-56a8-45be-8ffb-2ffa621c8281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      BASEL_humidity  BASEL_pressure  BASEL_sunshine  TOURS_temp_mean  \\\n",
              "0               0.89          1.0286             0.0              8.5   \n",
              "1               0.87          1.0318             0.0              7.9   \n",
              "2               0.81          1.0314             3.7              8.1   \n",
              "3               0.79          1.0262             6.9              8.6   \n",
              "4               0.90          1.0246             3.7              8.0   \n",
              "...              ...             ...             ...              ...   \n",
              "3649            0.82          1.0084             0.3              6.2   \n",
              "3650            0.92          1.0028             0.2             10.4   \n",
              "3651            0.92          0.9979             0.0             10.0   \n",
              "3652            0.93          0.9958             0.1              8.5   \n",
              "3653            0.93          0.9965             0.0              0.5   \n",
              "\n",
              "      TOURS_temp_min  TOURS_temp_max  \n",
              "0                7.2             9.8  \n",
              "1                6.6             9.2  \n",
              "2                6.6             9.6  \n",
              "3                6.4            10.8  \n",
              "4                6.4             9.5  \n",
              "...              ...             ...  \n",
              "3649             1.8            10.6  \n",
              "3650             6.2            14.5  \n",
              "3651             8.7            11.3  \n",
              "3652             6.2            10.9  \n",
              "3653            -0.7             1.8  \n",
              "\n",
              "[3654 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6a7635d-9c92-4061-9a96-2dfb1b342584\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BASEL_humidity</th>\n",
              "      <th>BASEL_pressure</th>\n",
              "      <th>BASEL_sunshine</th>\n",
              "      <th>TOURS_temp_mean</th>\n",
              "      <th>TOURS_temp_min</th>\n",
              "      <th>TOURS_temp_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.89</td>\n",
              "      <td>1.0286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7.2</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.87</td>\n",
              "      <td>1.0318</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.81</td>\n",
              "      <td>1.0314</td>\n",
              "      <td>3.7</td>\n",
              "      <td>8.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.79</td>\n",
              "      <td>1.0262</td>\n",
              "      <td>6.9</td>\n",
              "      <td>8.6</td>\n",
              "      <td>6.4</td>\n",
              "      <td>10.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.90</td>\n",
              "      <td>1.0246</td>\n",
              "      <td>3.7</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.4</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3649</th>\n",
              "      <td>0.82</td>\n",
              "      <td>1.0084</td>\n",
              "      <td>0.3</td>\n",
              "      <td>6.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3650</th>\n",
              "      <td>0.92</td>\n",
              "      <td>1.0028</td>\n",
              "      <td>0.2</td>\n",
              "      <td>10.4</td>\n",
              "      <td>6.2</td>\n",
              "      <td>14.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3651</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>11.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3652</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9958</td>\n",
              "      <td>0.1</td>\n",
              "      <td>8.5</td>\n",
              "      <td>6.2</td>\n",
              "      <td>10.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3653</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3654 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6a7635d-9c92-4061-9a96-2dfb1b342584')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6a7635d-9c92-4061-9a96-2dfb1b342584 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6a7635d-9c92-4061-9a96-2dfb1b342584');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [ 'BASEL_humidity' , 'BASEL_pressure' ,'BASEL_sunshine','TOURS_temp_mean','TOURS_temp_min']\n",
        "target = 'TOURS_temp_max'"
      ],
      "metadata": {
        "id": "EPXHf7PPt-H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast_ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewyHg0OUMlE1",
        "outputId": "9209845c-18c6-4ab3-e734-c17e6798daf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fast_ml\n",
            "  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fast_ml\n",
            "Successfully installed fast_ml-3.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= df_1.values[:, :-1]\n",
        "y= df_1.values[:, -1]"
      ],
      "metadata": {
        "id": "kes3kLVeXRJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryFMAgN9SFDr",
        "outputId": "30c9575f-76ee-4526-a8fe-c3b9edd29704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.89  ,  1.0286,  0.    ,  8.5   ,  7.2   ],\n",
              "       [ 0.87  ,  1.0318,  0.    ,  7.9   ,  6.6   ],\n",
              "       [ 0.81  ,  1.0314,  3.7   ,  8.1   ,  6.6   ],\n",
              "       ...,\n",
              "       [ 0.92  ,  0.9979,  0.    , 10.    ,  8.7   ],\n",
              "       [ 0.93  ,  0.9958,  0.1   ,  8.5   ,  6.2   ],\n",
              "       [ 0.93  ,  0.9965,  0.    ,  0.5   , -0.7   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdw-iEGBse6U",
        "outputId": "8faf62bb-7b75-40e2-937a-ff53461a65b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3654, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sml0PHGlSKF3",
        "outputId": "29602cf6-aba3-4e04-9d10-612ac240997a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9.8,  9.2,  9.6, ..., 11.3, 10.9,  1.8])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVWoKIyVsnXq",
        "outputId": "89ed5011-35bd-4c3d-8fa9-45815eb5a7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3654,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split Data**"
      ],
      "metadata": {
        "id": "GjsFINQt8czh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The data is in ascending order\n",
        "#from 2000 to 2007 = 2923 from 3654 so it will be = .8\n",
        "#and 2008 + 2009 = 731 from 3654 so it will be = .2\n",
        "#'shuffle = false' (it is mean that we take from the data( fist 2923==> train & after them will be validtion = 731))\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0, shuffle=False, train_size = .8)\n",
        "print(X_train.shape), print(y_train.shape)\n",
        "print(X_valid.shape), print(y_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c26cQJyXVapF",
        "outputId": "10078941-364d-43cb-c84d-c52bae644a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2923, 5)\n",
            "(2923,)\n",
            "(731, 5)\n",
            "(731,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step = 5\n",
        "#add step elements into train and validation\n",
        "train = np.append(X_train,np.repeat(X_train[-1],step))\n",
        "validation = np.append(X_valid,np.repeat(X_valid[-1],step))"
      ],
      "metadata": {
        "id": "qWtg0ys8yq5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.reshape( X_train , (X_train.shape[0],1 ,X_train.shape[1]))\n",
        "X_valid = np.reshape( X_valid , (X_valid.shape[0],1 ,X_valid.shape[1]))\n",
        "y_train = np.reshape( y_train , (y_train.shape[0],1))\n",
        "y_valid = np.reshape( y_valid , (y_valid.shape[0],1 ))\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-8dBfv_zaek",
        "outputId": "197a726e-0ae7-49d5-8cc4-155a9675bb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2923, 1, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Early Stopping**"
      ],
      "metadata": {
        "id": "dTlt2Q4Q8FPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)"
      ],
      "metadata": {
        "id": "tUIIj7tW8Ek_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Model**"
      ],
      "metadata": {
        "id": "I4EduQ5R7QMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bulding a model with simpleRNN\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=32, input_shape=(1,step),activation=\"tanh\"))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mse', optimizer= 'adam')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD1iPwXkrZqG",
        "outputId": "918647f3-ceac-47c3-8208-d2e775181562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 32)                1216      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,249\n",
            "Trainable params: 1,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model with train data and predict validation data\n",
        "model.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=150,batch_size=32,callbacks=[earlyStopping])\n",
        "trainPredict = model.predict(X_train)\n",
        "validPredict=model.predict(X_valid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGFI6LxqtJbR",
        "outputId": "43df758d-9821-4b9e-880b-19e6ea9e89e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "92/92 [==============================] - 7s 7ms/step - loss: 248.9670 - val_loss: 174.7501\n",
            "Epoch 2/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 130.9024 - val_loss: 95.0794\n",
            "Epoch 3/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 86.6367 - val_loss: 67.3562\n",
            "Epoch 4/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 64.1706 - val_loss: 50.2056\n",
            "Epoch 5/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 49.5825 - val_loss: 38.6925\n",
            "Epoch 6/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 38.9124 - val_loss: 29.9144\n",
            "Epoch 7/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 30.5866 - val_loss: 23.1282\n",
            "Epoch 8/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 24.2085 - val_loss: 18.0447\n",
            "Epoch 9/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 19.3253 - val_loss: 14.2073\n",
            "Epoch 10/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 15.5491 - val_loss: 11.3461\n",
            "Epoch 11/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 12.6185 - val_loss: 8.9315\n",
            "Epoch 12/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 10.3089 - val_loss: 7.1764\n",
            "Epoch 13/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 8.5413 - val_loss: 5.8119\n",
            "Epoch 14/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 7.1012 - val_loss: 4.7927\n",
            "Epoch 15/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 5.9901 - val_loss: 3.9252\n",
            "Epoch 16/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 5.0889 - val_loss: 3.2605\n",
            "Epoch 17/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 4.3587 - val_loss: 2.7263\n",
            "Epoch 18/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 3.7550 - val_loss: 2.3040\n",
            "Epoch 19/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 3.2647 - val_loss: 1.9568\n",
            "Epoch 20/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 2.8590 - val_loss: 1.7192\n",
            "Epoch 21/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 2.5264 - val_loss: 1.4772\n",
            "Epoch 22/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 2.2332 - val_loss: 1.2801\n",
            "Epoch 23/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.9912 - val_loss: 1.1443\n",
            "Epoch 24/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.7831 - val_loss: 1.0062\n",
            "Epoch 25/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.6101 - val_loss: 0.9879\n",
            "Epoch 26/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.4552 - val_loss: 0.8399\n",
            "Epoch 27/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.3272 - val_loss: 0.7151\n",
            "Epoch 28/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.2052 - val_loss: 0.7959\n",
            "Epoch 29/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.1025 - val_loss: 0.6148\n",
            "Epoch 30/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.0096 - val_loss: 0.5543\n",
            "Epoch 31/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.9274 - val_loss: 0.4944\n",
            "Epoch 32/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.8524 - val_loss: 0.4875\n",
            "Epoch 33/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.7883 - val_loss: 0.4157\n",
            "Epoch 34/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.7366 - val_loss: 0.4008\n",
            "Epoch 35/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.6768 - val_loss: 0.3581\n",
            "Epoch 36/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6277 - val_loss: 0.3255\n",
            "Epoch 37/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.5817 - val_loss: 0.2999\n",
            "Epoch 38/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5388 - val_loss: 0.2926\n",
            "Epoch 39/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4996 - val_loss: 0.2529\n",
            "Epoch 40/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4663 - val_loss: 0.2525\n",
            "Epoch 41/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4352 - val_loss: 0.2334\n",
            "Epoch 42/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4084 - val_loss: 0.2103\n",
            "Epoch 43/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3762 - val_loss: 0.1909\n",
            "Epoch 44/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3524 - val_loss: 0.1971\n",
            "Epoch 45/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3279 - val_loss: 0.1842\n",
            "Epoch 46/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3094 - val_loss: 0.1531\n",
            "Epoch 47/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2983 - val_loss: 0.1496\n",
            "Epoch 48/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2739 - val_loss: 0.1310\n",
            "Epoch 49/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2550 - val_loss: 0.1247\n",
            "Epoch 50/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2391 - val_loss: 0.1141\n",
            "Epoch 51/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2253 - val_loss: 0.1099\n",
            "Epoch 52/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2107 - val_loss: 0.1084\n",
            "Epoch 53/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 0.1064\n",
            "Epoch 54/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1844 - val_loss: 0.0841\n",
            "Epoch 55/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1765 - val_loss: 0.0817\n",
            "Epoch 56/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1661 - val_loss: 0.0731\n",
            "Epoch 57/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1576 - val_loss: 0.0689\n",
            "Epoch 58/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1459 - val_loss: 0.0701\n",
            "Epoch 59/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1368 - val_loss: 0.0639\n",
            "Epoch 60/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1293 - val_loss: 0.0606\n",
            "Epoch 61/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1220 - val_loss: 0.0608\n",
            "Epoch 62/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1149 - val_loss: 0.0512\n",
            "Epoch 63/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.0490\n",
            "Epoch 64/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1046 - val_loss: 0.0529\n",
            "Epoch 65/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0982 - val_loss: 0.0951\n",
            "Epoch 66/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.0435\n",
            "Epoch 67/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0895 - val_loss: 0.0431\n",
            "Epoch 68/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.0391\n",
            "Epoch 69/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.0461\n",
            "Epoch 70/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.0419\n",
            "Epoch 71/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0332\n",
            "Epoch 72/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0683 - val_loss: 0.0425\n",
            "Epoch 73/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0651 - val_loss: 0.0335\n",
            "Epoch 74/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0633 - val_loss: 0.0448\n",
            "Epoch 75/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0592 - val_loss: 0.0436\n",
            "Epoch 76/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0592 - val_loss: 0.0300\n",
            "Epoch 77/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0273\n",
            "Epoch 78/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0516 - val_loss: 0.0274\n",
            "Epoch 79/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0487 - val_loss: 0.0256\n",
            "Epoch 80/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0470 - val_loss: 0.0267\n",
            "Epoch 81/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0462 - val_loss: 0.0260\n",
            "Epoch 82/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0455 - val_loss: 0.0307\n",
            "Epoch 83/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.0225\n",
            "Epoch 84/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0405 - val_loss: 0.0220\n",
            "Epoch 85/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.0212\n",
            "Epoch 86/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0384 - val_loss: 0.0214\n",
            "Epoch 87/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0358 - val_loss: 0.0319\n",
            "Epoch 88/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0222\n",
            "Epoch 89/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0191\n",
            "Epoch 90/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0348 - val_loss: 0.0247\n",
            "Epoch 91/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.0198\n",
            "Epoch 92/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0303\n",
            "Epoch 93/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0300 - val_loss: 0.0214\n",
            "Epoch 94/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0180\n",
            "Epoch 95/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0196\n",
            "Epoch 96/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0255 - val_loss: 0.0168\n",
            "Epoch 97/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.0264\n",
            "Epoch 98/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.0431\n",
            "Epoch 99/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.0181\n",
            "Epoch 100/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.0208\n",
            "Epoch 101/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0222 - val_loss: 0.0148\n",
            "Epoch 102/150\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0234 - val_loss: 0.0145\n",
            "Epoch 103/150\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.0230 - val_loss: 0.0159\n",
            "Epoch 104/150\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0227 - val_loss: 0.0208\n",
            "Epoch 105/150\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0190 - val_loss: 0.0134\n",
            "Epoch 106/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0206 - val_loss: 0.0126\n",
            "Epoch 107/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0192 - val_loss: 0.0121\n",
            "Epoch 108/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0181 - val_loss: 0.0136\n",
            "Epoch 109/150\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0199 - val_loss: 0.0141\n",
            "Epoch 110/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0200 - val_loss: 0.0132\n",
            "Epoch 111/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0136\n",
            "Epoch 112/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0188 - val_loss: 0.0201\n",
            "Epoch 113/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0172 - val_loss: 0.0158\n",
            "Epoch 114/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0157 - val_loss: 0.0151\n",
            "Epoch 115/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0169 - val_loss: 0.0117\n",
            "Epoch 116/150\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0120\n",
            "Epoch 117/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0161 - val_loss: 0.0122\n",
            "Epoch 118/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0149 - val_loss: 0.0116\n",
            "Epoch 119/150\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0190 - val_loss: 0.0114\n",
            "Epoch 120/150\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.0146 - val_loss: 0.0131\n",
            "Epoch 121/150\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0165 - val_loss: 0.0139\n",
            "Epoch 122/150\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0137 - val_loss: 0.0131\n",
            "Epoch 123/150\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0134 - val_loss: 0.0108\n",
            "Epoch 124/150\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0129 - val_loss: 0.0100\n",
            "Epoch 125/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0139 - val_loss: 0.0097\n",
            "Epoch 126/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0139 - val_loss: 0.0100\n",
            "Epoch 127/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0132 - val_loss: 0.0106\n",
            "Epoch 128/150\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0148 - val_loss: 0.0299\n",
            "Epoch 129/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0142 - val_loss: 0.0105\n",
            "Epoch 130/150\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0136 - val_loss: 0.0117\n",
            "Epoch 131/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0104\n",
            "Epoch 132/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0125 - val_loss: 0.0098\n",
            "Epoch 133/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0133 - val_loss: 0.0187\n",
            "Epoch 134/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0150 - val_loss: 0.0151\n",
            "Epoch 135/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0164 - val_loss: 0.0107\n",
            "Epoch 135: early stopping\n",
            "92/92 [==============================] - 0s 3ms/step\n",
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try three different step sizes**"
      ],
      "metadata": {
        "id": "b0r84uoJ8k9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 3\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_valid_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "RNN_step = model.fit(X_train_step, y_train, validation_data=(X_valid_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'RNN validation loss: {RNN_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NNPlHsu4m9f",
        "outputId": "438b1f17-ec02-47fd-f247-d0c0cae2f1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.1680 - val_loss: 0.5883\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.5548 - val_loss: 0.5375\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.5150 - val_loss: 0.3767\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4227 - val_loss: 0.3780\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3711 - val_loss: 0.2493\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3470 - val_loss: 0.2645\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3103 - val_loss: 0.2022\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2932 - val_loss: 0.1877\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2650 - val_loss: 0.1873\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2476 - val_loss: 0.1690\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2192 - val_loss: 0.2645\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2384 - val_loss: 0.1382\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2001 - val_loss: 0.1306\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2013 - val_loss: 0.1246\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1755 - val_loss: 0.1757\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1647 - val_loss: 0.1032\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1567 - val_loss: 0.0996\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1431 - val_loss: 0.1133\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1371 - val_loss: 0.0836\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1296 - val_loss: 0.0939\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1217 - val_loss: 0.0851\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.0730\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1061 - val_loss: 0.0931\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.0867\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0997 - val_loss: 0.0750\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.0586\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0927 - val_loss: 0.0589\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0846 - val_loss: 0.0550\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0890 - val_loss: 0.0529\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0781 - val_loss: 0.0493\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0747 - val_loss: 0.0523\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0825 - val_loss: 0.0520\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0729 - val_loss: 0.0547\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0716 - val_loss: 0.0483\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0733 - val_loss: 0.0485\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0640 - val_loss: 0.0892\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0673 - val_loss: 0.0558\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0406\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0592 - val_loss: 0.0522\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0443\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.0549\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0429\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0551 - val_loss: 0.0372\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0546 - val_loss: 0.0493\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0575 - val_loss: 0.0688\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.1037\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.0361\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0540 - val_loss: 0.0736\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.0331\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.0389\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.0438\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0489 - val_loss: 0.0394\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0463 - val_loss: 0.0642\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0522 - val_loss: 0.1050\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0561 - val_loss: 0.0306\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.0584\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0446 - val_loss: 0.0285\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0436 - val_loss: 0.0542\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.0342\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0434 - val_loss: 0.0655\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0328\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0428 - val_loss: 0.0463\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0522 - val_loss: 0.0264\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0384 - val_loss: 0.0319\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.0391\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0377 - val_loss: 0.0285\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0387 - val_loss: 0.0268\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0370 - val_loss: 0.0244\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.0278\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0430 - val_loss: 0.1392\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0389 - val_loss: 0.0254\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0342 - val_loss: 0.0280\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0372 - val_loss: 0.0543\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0370 - val_loss: 0.0236\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0331 - val_loss: 0.0238\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0316 - val_loss: 0.0687\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0453 - val_loss: 0.0503\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0397 - val_loss: 0.0529\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.0260\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0260\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0203\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.0216\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.0206\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0297 - val_loss: 0.0261\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.0324 - val_loss: 0.0224\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0334\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0274 - val_loss: 0.0258\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0275 - val_loss: 0.0349\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0277 - val_loss: 0.0189\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.0272\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0274 - val_loss: 0.0194\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0331\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.0206\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.0192\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0249\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0281 - val_loss: 0.0187\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0310 - val_loss: 0.0190\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.0552\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0307\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0256 - val_loss: 0.0170\n",
            "RNN validation loss: 0.01699070818722248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 5\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_valid_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "RNN_step = model.fit(X_train_step, y_train, validation_data=(X_valid_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'RNN validation loss: {RNN_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyMSrlA07Xhp",
        "outputId": "10f3efd0-2b2f-4898-d6a6-6117270aa1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 9.6275 - val_loss: 1.9969\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.3848 - val_loss: 0.8859\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4391 - val_loss: 0.2146\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1311 - val_loss: 0.0719\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0679 - val_loss: 0.0403\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0487 - val_loss: 0.0271\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0414 - val_loss: 0.0261\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0217\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0321 - val_loss: 0.0192\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0159\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0269 - val_loss: 0.0164\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0231 - val_loss: 0.0170\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0217 - val_loss: 0.0146\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0211 - val_loss: 0.0146\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0200 - val_loss: 0.0178\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0198 - val_loss: 0.0167\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0191 - val_loss: 0.0137\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0183 - val_loss: 0.0143\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0182 - val_loss: 0.0151\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0175 - val_loss: 0.0128\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0126\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0153\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0166 - val_loss: 0.0134\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.0122\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.0111\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0148\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0150 - val_loss: 0.0113\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0144 - val_loss: 0.0111\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0141 - val_loss: 0.0115\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0135 - val_loss: 0.0114\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0117\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0103\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0092\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0090\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0113\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0088\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0083\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0092\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0083\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0084\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0097\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0079\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0086 - val_loss: 0.0102\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0091 - val_loss: 0.0085\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0095 - val_loss: 0.0084\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0082 - val_loss: 0.0118\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0086 - val_loss: 0.0070\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0078 - val_loss: 0.0066\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0079 - val_loss: 0.0092\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.0075\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0085 - val_loss: 0.0068\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0086 - val_loss: 0.0063\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0078 - val_loss: 0.0071\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0077 - val_loss: 0.0080\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0078 - val_loss: 0.0064\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0085 - val_loss: 0.0098\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0078 - val_loss: 0.0063\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0064\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0066\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0116\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0108\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.0067\n",
            "Epoch 62: early stopping\n",
            "RNN validation loss: 0.006671666167676449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 20\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_valid_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "RNN_step = model.fit(X_train_step, y_train, validation_data=(X_valid_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'RNN validation loss: {RNN_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_ITQ75Y7ZHJ",
        "outputId": "269d1287-c73b-4af9-9c88-5c3ee49bbec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 48.2679 - val_loss: 18.2533\n",
            "Epoch 2/100\n",
            "45/92 [=============>................] - ETA: 0s - loss: 18.8455Epoch 3/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 14.3471 - val_loss: 11.2868\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 9.3766 - val_loss: 5.9442\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 7.0577 - val_loss: 5.1583\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 6.1937 - val_loss: 4.6361\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 5.5858 - val_loss: 4.2546\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 5.0813 - val_loss: 3.8196\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 4.6614 - val_loss: 3.5205\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 3.5556 - val_loss: 1.5436\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 2.2190 - val_loss: 1.3154\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.8920 - val_loss: 1.1912\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.7003 - val_loss: 1.0984\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.5487 - val_loss: 1.0558\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.4353 - val_loss: 0.9628\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.1773 - val_loss: 0.6624\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.6696 - val_loss: 0.1984\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.3789 - val_loss: 0.1555\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.3380 - val_loss: 0.1300\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.3138 - val_loss: 0.1162\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2935 - val_loss: 0.1084\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1065\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2605 - val_loss: 0.1033\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2455 - val_loss: 0.0938\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2363 - val_loss: 0.0862\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2222 - val_loss: 0.0839\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2144 - val_loss: 0.0811\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2076 - val_loss: 0.0920\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1963 - val_loss: 0.0811\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1879 - val_loss: 0.0768\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1811 - val_loss: 0.0726\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1723 - val_loss: 0.0769\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1676 - val_loss: 0.0664\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1612 - val_loss: 0.0677\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1553 - val_loss: 0.0637\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1487 - val_loss: 0.0629\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1455 - val_loss: 0.0604\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1375 - val_loss: 0.0873\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1342 - val_loss: 0.0553\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1267 - val_loss: 0.0541\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1224 - val_loss: 0.0662\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1221 - val_loss: 0.0587\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1130 - val_loss: 0.0468\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1091 - val_loss: 0.0506\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1064 - val_loss: 0.0435\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1001 - val_loss: 0.0444\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.0395\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0932 - val_loss: 0.0396\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0922 - val_loss: 0.0485\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0851 - val_loss: 0.0385\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0814 - val_loss: 0.0345\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.0352\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0772 - val_loss: 0.0385\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0434\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0685 - val_loss: 0.0365\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0673 - val_loss: 0.0292\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0657 - val_loss: 0.0304\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0551 - val_loss: 0.0333\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0499 - val_loss: 0.0274\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0487 - val_loss: 0.0427\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0329\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0439 - val_loss: 0.0317\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0424 - val_loss: 0.0362\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.0215\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0264\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0207\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.0321\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.0265\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0182\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0308\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0170\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0155\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0157\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0233 - val_loss: 0.0187\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0232 - val_loss: 0.0202\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0217 - val_loss: 0.0223\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0131\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.0126\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.0126\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0193 - val_loss: 0.0184\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.0147\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0199 - val_loss: 0.0115\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 0.0154\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0180 - val_loss: 0.0261\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.0128\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0116\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 0.0152\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0168 - val_loss: 0.0109\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0154 - val_loss: 0.0168\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0179 - val_loss: 0.0109\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0122\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0164 - val_loss: 0.0124\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.0103\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.0107\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0168\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.0159\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0147 - val_loss: 0.0168\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0159\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0147 - val_loss: 0.0120\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0140\n",
            "RNN validation loss: 0.013997949659824371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot (1716).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "xk1L9A7wjK9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU Model**"
      ],
      "metadata": {
        "id": "uiq7wam77eHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bulding a model with GRU\n",
        "model_1 = Sequential()\n",
        "model_1.add(GRU(units=32, input_shape=(1,step),activation=\"tanh\"))\n",
        "model_1.add(Dense(8,activation= \"relu\"))\n",
        "model_1.add(Dense(1))\n",
        "model_1.compile(loss='mse', optimizer= 'adam')\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yJ2ed96VOjm",
        "outputId": "736916d9-23de-4935-b975-c50e8f419c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 32)                3744      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 264       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,017\n",
            "Trainable params: 4,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model with train data and predict validation data\n",
        "model_1.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=100,batch_size=32,callbacks=[earlyStopping])\n",
        "trainPredict = model_1.predict(X_train)\n",
        "validPredict=model_1.predict(X_valid)\n",
        "predicted=np.concatenate((trainPredict,validPredict),axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoyOrR9Sg0mo",
        "outputId": "6a6f64eb-4abc-4a5f-c033-31335579725a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 4s 8ms/step - loss: 229.2979 - val_loss: 119.6713\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 56.5262 - val_loss: 21.0612\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 16.6809 - val_loss: 8.1211\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 6.6807 - val_loss: 3.0017\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 2.9478 - val_loss: 1.5071\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 1.7386 - val_loss: 0.9730\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.1716 - val_loss: 0.6261\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.8468 - val_loss: 0.4556\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6404 - val_loss: 0.3503\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.2480\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4002 - val_loss: 0.2241\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3313 - val_loss: 0.1631\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.1352\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2270 - val_loss: 0.1718\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1958 - val_loss: 0.1107\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1643 - val_loss: 0.1038\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1421 - val_loss: 0.0955\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1258 - val_loss: 0.0939\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1119 - val_loss: 0.0622\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.0545\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0954 - val_loss: 0.0510\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0804 - val_loss: 0.0657\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0452\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.0423\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0645 - val_loss: 0.0427\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0564 - val_loss: 0.0359\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0525 - val_loss: 0.0332\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0538 - val_loss: 0.0302\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0561 - val_loss: 0.0307\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0476 - val_loss: 0.0279\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0414 - val_loss: 0.0282\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0381 - val_loss: 0.0289\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0372 - val_loss: 0.0311\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0345 - val_loss: 0.0244\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0326 - val_loss: 0.0234\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0355 - val_loss: 0.0313\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.0220\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0322 - val_loss: 0.0213\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0243\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0405\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0200\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0231 - val_loss: 0.0177\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0245 - val_loss: 0.0166\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0233 - val_loss: 0.0184\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0236 - val_loss: 0.0285\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0333 - val_loss: 0.0168\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0203 - val_loss: 0.0332\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0212\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0242 - val_loss: 0.0168\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0266\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0210 - val_loss: 0.0180\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0130\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0224 - val_loss: 0.0156\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0233\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0149\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0120\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0126\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0188\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0165 - val_loss: 0.0115\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0149 - val_loss: 0.0216\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0228 - val_loss: 0.0112\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0145 - val_loss: 0.0145\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0152\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0152\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0101\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0271\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0198\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0149\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0141 - val_loss: 0.0088\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0095\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0227\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0098\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0195\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0088\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0193\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0112\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0089\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0099\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0152 - val_loss: 0.0079\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0095\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0108\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0220\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0113\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0117\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0341\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0134\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0099\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0087\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0209\n",
            "Epoch 90: early stopping\n",
            "92/92 [==============================] - 0s 2ms/step\n",
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try three different step sizes**"
      ],
      "metadata": {
        "id": "Wmj9VBVD8y90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 3\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_val_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "GRU_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'GRU validation loss: {GRU_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHHsZ9RQ6bIq",
        "outputId": "4b8e3eb6-4593-406d-995d-7dd845142d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 91.5429 - val_loss: 1.7877\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.1872 - val_loss: 0.5104\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4273 - val_loss: 0.2495\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1751 - val_loss: 0.1341\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1233 - val_loss: 0.0973\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1062 - val_loss: 0.0851\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0934 - val_loss: 0.0804\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0870 - val_loss: 0.0768\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0787 - val_loss: 0.0678\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0696\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0676 - val_loss: 0.0548\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0628 - val_loss: 0.0510\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0606 - val_loss: 0.0596\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0560 - val_loss: 0.0453\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0547 - val_loss: 0.0487\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0524 - val_loss: 0.0463\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0501 - val_loss: 0.0632\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0481 - val_loss: 0.0392\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0481 - val_loss: 0.0448\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.0415\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.0354\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0449 - val_loss: 0.0348\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0477 - val_loss: 0.0371\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0434 - val_loss: 0.0378\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0431 - val_loss: 0.0348\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0425 - val_loss: 0.0353\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.0372\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0407 - val_loss: 0.0413\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0424 - val_loss: 0.0342\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0402 - val_loss: 0.0348\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0339\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0394 - val_loss: 0.0357\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0387 - val_loss: 0.0370\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.0303\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0383 - val_loss: 0.0307\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.0325\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0376 - val_loss: 0.0305\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0374 - val_loss: 0.0306\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0392 - val_loss: 0.0278\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0352 - val_loss: 0.0376\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0400 - val_loss: 0.0285\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0286\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0354 - val_loss: 0.0283\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0355 - val_loss: 0.0277\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 0.0291\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0380 - val_loss: 0.0299\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0352 - val_loss: 0.0304\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0282\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0350 - val_loss: 0.0276\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0371 - val_loss: 0.0262\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.0283\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 0.0254\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0282\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0324 - val_loss: 0.0315\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0355\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0259\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0335 - val_loss: 0.0239\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0327 - val_loss: 0.0398\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0323 - val_loss: 0.0385\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0339 - val_loss: 0.0230\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0326 - val_loss: 0.0294\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0329 - val_loss: 0.0235\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.0228\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0306 - val_loss: 0.0262\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0315 - val_loss: 0.0245\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0260\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0328 - val_loss: 0.0224\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0329 - val_loss: 0.0234\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0309 - val_loss: 0.0222\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0304 - val_loss: 0.0300\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.0226\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0315 - val_loss: 0.0233\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0294 - val_loss: 0.0230\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0300 - val_loss: 0.0255\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0305 - val_loss: 0.0257\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0308 - val_loss: 0.0221\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0288 - val_loss: 0.0199\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0278 - val_loss: 0.0192\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0191\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0208\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0196\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0250\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.0273\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0281 - val_loss: 0.0225\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0282 - val_loss: 0.0198\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.0225\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0197\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0261 - val_loss: 0.0191\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0257 - val_loss: 0.0185\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0255 - val_loss: 0.0187\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0251 - val_loss: 0.0204\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0259 - val_loss: 0.0172\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0193\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0260 - val_loss: 0.0161\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 0.0178\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.0189\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0254 - val_loss: 0.0178\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0220\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0224 - val_loss: 0.0294\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.0188\n",
            "GRU validation loss: 0.018832355737686157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 5\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_val_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "GRU_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'GRU validation loss: {GRU_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TiUbbm7oMf",
        "outputId": "df3d07f6-c72b-4359-c46f-8985c6be6946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0221 - val_loss: 0.0177\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0225 - val_loss: 0.0184\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0151\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0223 - val_loss: 0.0220\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0227 - val_loss: 0.0232\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0244 - val_loss: 0.0160\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0144\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0229 - val_loss: 0.0154\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0203 - val_loss: 0.0165\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0170\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0223 - val_loss: 0.0159\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0207 - val_loss: 0.0156\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0201 - val_loss: 0.0140\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.0128\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.0163\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0221 - val_loss: 0.0162\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0202 - val_loss: 0.0139\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.0246\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0198 - val_loss: 0.0140\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0204 - val_loss: 0.0138\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0201 - val_loss: 0.0150\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.0133\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0184 - val_loss: 0.0136\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0206 - val_loss: 0.0122\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0308\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0219 - val_loss: 0.0128\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0182 - val_loss: 0.0150\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0130\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0134\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0188 - val_loss: 0.0171\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0194 - val_loss: 0.0120\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0140\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0174 - val_loss: 0.0132\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0189 - val_loss: 0.0113\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.0168\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.0147\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0155\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0110\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0184 - val_loss: 0.0129\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.0141\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0182\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0197\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0121\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0199\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.0104\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.0117\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0132\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0162 - val_loss: 0.0191\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0112\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0101\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.0098\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0098\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.0108\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.0165\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.0145\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0098\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0119\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0137 - val_loss: 0.0105\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0154 - val_loss: 0.0127\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0163 - val_loss: 0.0102\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0199\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0095\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0118\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0182\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0152 - val_loss: 0.0102\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0297\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0149\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0095\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0095\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0129\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0130\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0145 - val_loss: 0.0097\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0100\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0093\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0117\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0092\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0093\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0090\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0139\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0139\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0141\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0142 - val_loss: 0.0122\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0087\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0131 - val_loss: 0.0096\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0122 - val_loss: 0.0126\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0150 - val_loss: 0.0092\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0140 - val_loss: 0.0092\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0105\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0100\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0139 - val_loss: 0.0087\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0082\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0149 - val_loss: 0.0100\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0119\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0082\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0087\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0119\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0095\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0091\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0076\n",
            "GRU validation loss: 0.007595114875584841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 20\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_val_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "GRU_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'GRU validation loss: {GRU_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucuP5H_67pzk",
        "outputId": "3f8c9a77-a2c5-439a-adbd-b2d0004cb711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0121\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0150\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0085\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0140 - val_loss: 0.0083\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0088\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0142\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0235\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0188\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0120\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0104\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0161\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0126 - val_loss: 0.0094\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.0086\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0197\n",
            "Epoch 14: early stopping\n",
            "GRU validation loss: 0.019746195524930954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot (1717).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "oyUIPI3bjYd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM Model**"
      ],
      "metadata": {
        "id": "y7fFbxxR7sQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bulding a model with LSTM\n",
        "model_2 = Sequential()\n",
        "model_2.add(LSTM(units=32, input_shape=(1,step),activation=\"tanh\"))\n",
        "model_2.add(Dense(8,activation= \"relu\"))\n",
        "model_2.add(Dense(1))\n",
        "model_2.compile(loss='mse', optimizer= 'adam')\n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6KcqZ-WVtVo",
        "outputId": "c1cd49a3-50b2-41c3-9bac-b4b7be6ab60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 32)                4864      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 264       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,137\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model with train data and predict validation data\n",
        "model_2.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=100,batch_size=32,callbacks=[earlyStopping])\n",
        "trainPredict = model_2.predict(X_train)\n",
        "validPredict=model_2.predict(X_valid)\n",
        "predicted=np.concatenate((trainPredict,validPredict),axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeIWhdgNhJ4f",
        "outputId": "8e167c27-94cb-4b8d-a277-9540dd19b348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 3s 10ms/step - loss: 290.9623 - val_loss: 180.7552\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 86.0596 - val_loss: 24.5619\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 20.1727 - val_loss: 10.5136\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 8.5447 - val_loss: 3.9529\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 3.8206 - val_loss: 2.0506\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 2.2530 - val_loss: 1.2666\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.5547 - val_loss: 1.1374\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.1549 - val_loss: 0.6375\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.8756 - val_loss: 0.5749\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.6873 - val_loss: 0.3755\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.5355 - val_loss: 0.2919\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4355 - val_loss: 0.2357\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3602 - val_loss: 0.2102\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3020 - val_loss: 0.1685\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2587 - val_loss: 0.1497\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2217 - val_loss: 0.1239\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1967 - val_loss: 0.1116\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1754 - val_loss: 0.1222\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1493 - val_loss: 0.1223\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1348 - val_loss: 0.0791\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1193 - val_loss: 0.0683\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1102 - val_loss: 0.0635\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0942 - val_loss: 0.0669\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0869 - val_loss: 0.0540\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0800 - val_loss: 0.0529\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0744 - val_loss: 0.0471\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0711 - val_loss: 0.0536\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0631 - val_loss: 0.0413\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0589 - val_loss: 0.0488\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0353\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0552 - val_loss: 0.0346\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0488 - val_loss: 0.0503\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0571 - val_loss: 0.0474\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0515 - val_loss: 0.0290\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0407 - val_loss: 0.0251\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0504\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0233\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0353 - val_loss: 0.0219\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0373 - val_loss: 0.0277\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0372 - val_loss: 0.0214\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0330 - val_loss: 0.0197\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0302 - val_loss: 0.0261\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.0206\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0173\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0179\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0293 - val_loss: 0.0276\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0216\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.0169\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.0141\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0210 - val_loss: 0.0132\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0124\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.0215\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0194 - val_loss: 0.0182\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.0114\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0198 - val_loss: 0.0129\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0134\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0113\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.0178\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0215 - val_loss: 0.0208\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0191 - val_loss: 0.0292\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0295\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0156 - val_loss: 0.0126\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0198 - val_loss: 0.0203\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0153 - val_loss: 0.0094\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0138 - val_loss: 0.0097\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0158 - val_loss: 0.0250\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0145\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0092\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.0344\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.0115\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0097\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0076\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0073\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0122\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0188 - val_loss: 0.0087\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0083\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0083\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0100\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0073\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0073\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0082\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0074\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0087\n",
            "Epoch 83: early stopping\n",
            "92/92 [==============================] - 0s 2ms/step\n",
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try three different step sizes**"
      ],
      "metadata": {
        "id": "POYrVCxE8-o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 3\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_val_step = np.cumsum(X_valid, axis=1) - step_size +1\n",
        "\n",
        "LSTM_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'LSTM validation loss: {LSTM_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxKvNFC97vHS",
        "outputId": "290299cd-84a3-4978-8926-de832d0e7be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0111\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0126\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0092\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0086\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0081\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0075\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0094\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0170\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0071\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0081\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0121\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0099\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0072\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0069\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0099\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0079\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0080\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0217\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0143\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0093\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0136\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0076\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0092\n",
            "Epoch 24: early stopping\n",
            "LSTM validation loss: 0.009185767732560635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 5\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size +1 \n",
        "X_val_step = np.cumsum(X_valid, axis=1) - step_size + 1\n",
        "\n",
        "LSTM_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'LSTM validation loss: {LSTM_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQs6xVHK71Is",
        "outputId": "a3adcb54-9f0f-42e2-b288-8528f98eaa5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0091\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0267\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0078\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0094\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0095\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0116 - val_loss: 0.0126\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0097 - val_loss: 0.0106\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0114 - val_loss: 0.0126\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0077\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0076\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0143\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0107\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0162\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0125 - val_loss: 0.0076\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0107\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0123\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0114\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0071\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0094\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0191\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0092\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0079\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0148\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0101 - val_loss: 0.0368\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0158 - val_loss: 0.0080\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0079\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0069\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0065\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0068\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0149\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0065\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0118 - val_loss: 0.0074\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0066\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0083\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0063\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0096\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0098\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0148\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0066\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0241\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0092\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0090\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0078\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0166\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0085\n",
            "Epoch 48: early stopping\n",
            "LSTM validation loss: 0.00853755883872509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 20\n",
        "X_train_step = np.cumsum(X_train, axis=1) - step_size + 1\n",
        "X_val_step = np.cumsum(X_valid, axis=1) - step_size +1\n",
        "\n",
        "LSTM_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid ), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'LSTM validation loss: {LSTM_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ6YFGRf73HN",
        "outputId": "835be4c4-014c-42ee-de47-ef1816b9de2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0071\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0065\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0085\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0154\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0303\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0095\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0121\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0118\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0065\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0078\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0069\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0264\n",
            "Epoch 12: early stopping\n",
            "LSTM validation loss: 0.02636992372572422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot (1718).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "LSoxc6xVjg9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion "
      ],
      "metadata": {
        "id": "UiHPF0S1Zymp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end we conclude that with different timesteps (3,5,20)\n",
        "We found that the model **LSTM** is better in terms of val_loss whe we use timeste= **3**\n",
        ", model **RNN** is better in terms of val_loss whe we use timeste= **5**\n",
        "and found that model **RNN** is better in terms of val_loss whe we use timeste= **20**"
      ],
      "metadata": {
        "id": "SB_5VJn3aG-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot (1719).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "SGt3hvAhW1RM"
      }
    }
  ]
}
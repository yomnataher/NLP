{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oUFaoVoHDYR",
        "outputId": "8fe033c1-9c62-44a1-afec-08e589b4cc7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense,SimpleRNN,LSTM, GRU\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "hSs0KEvYtj2a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Data**"
      ],
      "metadata": {
        "id": "uakwZAEB8Lo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/dataset/weather_prediction_dataset.csv')\n"
      ],
      "metadata": {
        "id": "ub4dsSqfH-Uo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4eXWhSIwKkjV",
        "outputId": "3a7febab-0cfe-40cc-b4ea-ff5eb6aa34d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          DATE  MONTH  BASEL_cloud_cover  BASEL_humidity  BASEL_pressure  \\\n",
              "0     20000101      1                  8            0.89          1.0286   \n",
              "1     20000102      1                  8            0.87          1.0318   \n",
              "2     20000103      1                  5            0.81          1.0314   \n",
              "3     20000104      1                  7            0.79          1.0262   \n",
              "4     20000105      1                  5            0.90          1.0246   \n",
              "...        ...    ...                ...             ...             ...   \n",
              "3649  20091228     12                  7            0.82          1.0084   \n",
              "3650  20091229     12                  7            0.92          1.0028   \n",
              "3651  20091230     12                  8            0.92          0.9979   \n",
              "3652  20091231     12                  7            0.93          0.9958   \n",
              "3653  20100101      1                  8            0.93          0.9965   \n",
              "\n",
              "      BASEL_global_radiation  BASEL_precipitation  BASEL_sunshine  \\\n",
              "0                       0.20                 0.03             0.0   \n",
              "1                       0.25                 0.00             0.0   \n",
              "2                       0.50                 0.00             3.7   \n",
              "3                       0.63                 0.35             6.9   \n",
              "4                       0.51                 0.07             3.7   \n",
              "...                      ...                  ...             ...   \n",
              "3649                    0.28                 0.42             0.3   \n",
              "3650                    0.22                 1.68             0.2   \n",
              "3651                    0.07                 1.54             0.0   \n",
              "3652                    0.17                 0.57             0.1   \n",
              "3653                    0.08                 0.56             0.0   \n",
              "\n",
              "      BASEL_temp_mean  BASEL_temp_min  ...  STOCKHOLM_temp_min  \\\n",
              "0                 2.9             1.6  ...                -9.3   \n",
              "1                 3.6             2.7  ...                 0.5   \n",
              "2                 2.2             0.1  ...                -1.0   \n",
              "3                 3.9             0.5  ...                 2.5   \n",
              "4                 6.0             3.8  ...                -1.8   \n",
              "...               ...             ...  ...                 ...   \n",
              "3649              3.2             1.0  ...                -2.7   \n",
              "3650              4.5             2.4  ...                -9.5   \n",
              "3651              8.5             7.5  ...               -12.5   \n",
              "3652              6.6             4.3  ...                -9.3   \n",
              "3653              2.9            -0.2  ...                -8.8   \n",
              "\n",
              "      STOCKHOLM_temp_max  TOURS_wind_speed  TOURS_humidity  TOURS_pressure  \\\n",
              "0                    0.7               1.6            0.97          1.0275   \n",
              "1                    2.0               2.0            0.99          1.0293   \n",
              "2                    2.8               3.4            0.91          1.0267   \n",
              "3                    4.6               4.9            0.95          1.0222   \n",
              "4                    2.9               3.6            0.95          1.0209   \n",
              "...                  ...               ...             ...             ...   \n",
              "3649                 2.4               3.7            0.95          1.0011   \n",
              "3650                 0.8               5.3            0.89          0.9966   \n",
              "3651                -7.4               3.8            0.88          0.9939   \n",
              "3652                -6.5               4.2            0.88          0.9933   \n",
              "3653                -7.0               3.4            0.86          1.0040   \n",
              "\n",
              "      TOURS_global_radiation  TOURS_precipitation  TOURS_temp_mean  \\\n",
              "0                       0.25                 0.04              8.5   \n",
              "1                       0.17                 0.16              7.9   \n",
              "2                       0.27                 0.00              8.1   \n",
              "3                       0.11                 0.44              8.6   \n",
              "4                       0.39                 0.04              8.0   \n",
              "...                      ...                  ...              ...   \n",
              "3649                    0.22                 1.50              6.2   \n",
              "3650                    0.24                 0.40             10.4   \n",
              "3651                    0.24                 1.00             10.0   \n",
              "3652                    0.58                 0.02              8.5   \n",
              "3653                    0.11                 0.00              0.5   \n",
              "\n",
              "      TOURS_temp_min  TOURS_temp_max  \n",
              "0                7.2             9.8  \n",
              "1                6.6             9.2  \n",
              "2                6.6             9.6  \n",
              "3                6.4            10.8  \n",
              "4                6.4             9.5  \n",
              "...              ...             ...  \n",
              "3649             1.8            10.6  \n",
              "3650             6.2            14.5  \n",
              "3651             8.7            11.3  \n",
              "3652             6.2            10.9  \n",
              "3653            -0.7             1.8  \n",
              "\n",
              "[3654 rows x 165 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79f249bf-5be4-42c9-8032-36c6b8b7a329\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>MONTH</th>\n",
              "      <th>BASEL_cloud_cover</th>\n",
              "      <th>BASEL_humidity</th>\n",
              "      <th>BASEL_pressure</th>\n",
              "      <th>BASEL_global_radiation</th>\n",
              "      <th>BASEL_precipitation</th>\n",
              "      <th>BASEL_sunshine</th>\n",
              "      <th>BASEL_temp_mean</th>\n",
              "      <th>BASEL_temp_min</th>\n",
              "      <th>...</th>\n",
              "      <th>STOCKHOLM_temp_min</th>\n",
              "      <th>STOCKHOLM_temp_max</th>\n",
              "      <th>TOURS_wind_speed</th>\n",
              "      <th>TOURS_humidity</th>\n",
              "      <th>TOURS_pressure</th>\n",
              "      <th>TOURS_global_radiation</th>\n",
              "      <th>TOURS_precipitation</th>\n",
              "      <th>TOURS_temp_mean</th>\n",
              "      <th>TOURS_temp_min</th>\n",
              "      <th>TOURS_temp_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20000101</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.89</td>\n",
              "      <td>1.0286</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.6</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1.0275</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7.2</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20000102</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.87</td>\n",
              "      <td>1.0318</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.0293</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.16</td>\n",
              "      <td>7.9</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20000103</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1.0314</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.7</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.91</td>\n",
              "      <td>1.0267</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20000104</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.0262</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.35</td>\n",
              "      <td>6.9</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.9</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.0222</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.44</td>\n",
              "      <td>8.6</td>\n",
              "      <td>6.4</td>\n",
              "      <td>10.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20000105</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.0246</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.07</td>\n",
              "      <td>3.7</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.8</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.0209</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.04</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.4</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3649</th>\n",
              "      <td>20091228</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.0084</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.7</td>\n",
              "      <td>2.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.0011</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.50</td>\n",
              "      <td>6.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3650</th>\n",
              "      <td>20091229</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1.0028</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2.4</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.5</td>\n",
              "      <td>0.8</td>\n",
              "      <td>5.3</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.40</td>\n",
              "      <td>10.4</td>\n",
              "      <td>6.2</td>\n",
              "      <td>14.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3651</th>\n",
              "      <td>20091230</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7.5</td>\n",
              "      <td>...</td>\n",
              "      <td>-12.5</td>\n",
              "      <td>-7.4</td>\n",
              "      <td>3.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9939</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.00</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>11.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3652</th>\n",
              "      <td>20091231</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9958</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>4.3</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>-6.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9933</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.02</td>\n",
              "      <td>8.5</td>\n",
              "      <td>6.2</td>\n",
              "      <td>10.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3653</th>\n",
              "      <td>20100101</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.8</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.0040</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3654 rows × 165 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79f249bf-5be4-42c9-8032-36c6b8b7a329')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79f249bf-5be4-42c9-8032-36c6b8b7a329 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79f249bf-5be4-42c9-8032-36c6b8b7a329');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**"
      ],
      "metadata": {
        "id": "imqql80x8Pgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['BASEL_humidity' , 'BASEL_pressure' ,'BASEL_sunshine','TOURS_temp_mean','TOURS_temp_min', 'TOURS_temp_max']\n",
        "df_1 = df[features]"
      ],
      "metadata": {
        "id": "6oXvSctHQo3g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aToQFxJ0RU4V",
        "outputId": "b963ffda-1ffc-4a12-915a-5e4d5a322cd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      BASEL_humidity  BASEL_pressure  BASEL_sunshine  TOURS_temp_mean  \\\n",
              "0               0.89          1.0286             0.0              8.5   \n",
              "1               0.87          1.0318             0.0              7.9   \n",
              "2               0.81          1.0314             3.7              8.1   \n",
              "3               0.79          1.0262             6.9              8.6   \n",
              "4               0.90          1.0246             3.7              8.0   \n",
              "...              ...             ...             ...              ...   \n",
              "3649            0.82          1.0084             0.3              6.2   \n",
              "3650            0.92          1.0028             0.2             10.4   \n",
              "3651            0.92          0.9979             0.0             10.0   \n",
              "3652            0.93          0.9958             0.1              8.5   \n",
              "3653            0.93          0.9965             0.0              0.5   \n",
              "\n",
              "      TOURS_temp_min  TOURS_temp_max  \n",
              "0                7.2             9.8  \n",
              "1                6.6             9.2  \n",
              "2                6.6             9.6  \n",
              "3                6.4            10.8  \n",
              "4                6.4             9.5  \n",
              "...              ...             ...  \n",
              "3649             1.8            10.6  \n",
              "3650             6.2            14.5  \n",
              "3651             8.7            11.3  \n",
              "3652             6.2            10.9  \n",
              "3653            -0.7             1.8  \n",
              "\n",
              "[3654 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae222803-658b-451c-8f73-c2cf936ef10a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BASEL_humidity</th>\n",
              "      <th>BASEL_pressure</th>\n",
              "      <th>BASEL_sunshine</th>\n",
              "      <th>TOURS_temp_mean</th>\n",
              "      <th>TOURS_temp_min</th>\n",
              "      <th>TOURS_temp_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.89</td>\n",
              "      <td>1.0286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7.2</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.87</td>\n",
              "      <td>1.0318</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.81</td>\n",
              "      <td>1.0314</td>\n",
              "      <td>3.7</td>\n",
              "      <td>8.1</td>\n",
              "      <td>6.6</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.79</td>\n",
              "      <td>1.0262</td>\n",
              "      <td>6.9</td>\n",
              "      <td>8.6</td>\n",
              "      <td>6.4</td>\n",
              "      <td>10.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.90</td>\n",
              "      <td>1.0246</td>\n",
              "      <td>3.7</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.4</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3649</th>\n",
              "      <td>0.82</td>\n",
              "      <td>1.0084</td>\n",
              "      <td>0.3</td>\n",
              "      <td>6.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>10.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3650</th>\n",
              "      <td>0.92</td>\n",
              "      <td>1.0028</td>\n",
              "      <td>0.2</td>\n",
              "      <td>10.4</td>\n",
              "      <td>6.2</td>\n",
              "      <td>14.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3651</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>11.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3652</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9958</td>\n",
              "      <td>0.1</td>\n",
              "      <td>8.5</td>\n",
              "      <td>6.2</td>\n",
              "      <td>10.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3653</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.9965</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3654 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae222803-658b-451c-8f73-c2cf936ef10a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae222803-658b-451c-8f73-c2cf936ef10a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae222803-658b-451c-8f73-c2cf936ef10a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [ 'BASEL_humidity' , 'BASEL_pressure' ,'BASEL_sunshine','TOURS_temp_mean','TOURS_temp_min']\n",
        "target = 'TOURS_temp_max'"
      ],
      "metadata": {
        "id": "EPXHf7PPt-H8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast_ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewyHg0OUMlE1",
        "outputId": "58ad7492-e4c6-4c67-900f-9a3d952f6bd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fast_ml\n",
            "  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fast_ml\n",
            "Successfully installed fast_ml-3.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= df_1.values[:, :-1]\n",
        "y= df_1.values[:, -1]"
      ],
      "metadata": {
        "id": "kes3kLVeXRJm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryFMAgN9SFDr",
        "outputId": "9b3f764d-010c-480c-e127-84fbdc806d14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.89  ,  1.0286,  0.    ,  8.5   ,  7.2   ],\n",
              "       [ 0.87  ,  1.0318,  0.    ,  7.9   ,  6.6   ],\n",
              "       [ 0.81  ,  1.0314,  3.7   ,  8.1   ,  6.6   ],\n",
              "       ...,\n",
              "       [ 0.92  ,  0.9979,  0.    , 10.    ,  8.7   ],\n",
              "       [ 0.93  ,  0.9958,  0.1   ,  8.5   ,  6.2   ],\n",
              "       [ 0.93  ,  0.9965,  0.    ,  0.5   , -0.7   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdw-iEGBse6U",
        "outputId": "71b64948-6c5b-47be-8189-0615d08bd623"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3654, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sml0PHGlSKF3",
        "outputId": "c257f901-fba4-42b0-973e-b3440cb81025"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9.8,  9.2,  9.6, ..., 11.3, 10.9,  1.8])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVWoKIyVsnXq",
        "outputId": "ac575282-9375-4c13-b6ee-0de596bf6aa9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3654,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split Data**"
      ],
      "metadata": {
        "id": "GjsFINQt8czh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from 2000 to 2007 = 2923\n",
        "#and 2008 + 2009 = 731 \n",
        "#'shuffle = false' (it is mean that we take from the data( fist 2923==> train & after them will be validtion = 731))\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0, shuffle=False, train_size = .8)\n",
        "print(X_train.shape), print(y_train.shape)\n",
        "print(X_valid.shape), print(y_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c26cQJyXVapF",
        "outputId": "4f6d81df-2fba-4482-9381-8aa3700ce852"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2923, 5)\n",
            "(2923,)\n",
            "(731, 5)\n",
            "(731,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step = 5\n",
        "#add step elements into train and validation\n",
        "train = np.append(X_train,np.repeat(X_train[-1],step))\n",
        "validation = np.append(X_valid,np.repeat(X_valid[-1],step))"
      ],
      "metadata": {
        "id": "qWtg0ys8yq5h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.reshape( X_train , (X_train.shape[0],1 ,X_train.shape[1]))\n",
        "X_valid = np.reshape( X_valid , (X_valid.shape[0],1 ,X_valid.shape[1]))\n",
        "y_train = np.reshape( y_train , (y_train.shape[0],1))\n",
        "y_valid = np.reshape( y_valid , (y_valid.shape[0],1 ))\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-8dBfv_zaek",
        "outputId": "2d28e833-95f3-4ba7-c7c3-a583d61a1970"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2923, 1, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Early Stopping**"
      ],
      "metadata": {
        "id": "dTlt2Q4Q8FPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)"
      ],
      "metadata": {
        "id": "tUIIj7tW8Ek_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Model**"
      ],
      "metadata": {
        "id": "I4EduQ5R7QMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bulding a model with simpleRNN\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=32, input_shape=(1,step),activation=\"tanh\"))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mse', optimizer= 'adam')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD1iPwXkrZqG",
        "outputId": "9359f23f-1a5d-453e-99a0-dfebf5f7e95b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 32)                1216      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,249\n",
            "Trainable params: 1,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model with train data and predict validation data\n",
        "model.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=150,batch_size=32,callbacks=[earlyStopping])\n",
        "trainPredict = model.predict(X_train)\n",
        "validPredict=model.predict(X_valid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGFI6LxqtJbR",
        "outputId": "3b2c7f9c-8c10-49e7-e125-9b0a677db71e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "92/92 [==============================] - 7s 7ms/step - loss: 257.0916 - val_loss: 178.4043\n",
            "Epoch 2/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 146.1783 - val_loss: 108.3014\n",
            "Epoch 3/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 91.8797 - val_loss: 69.5910\n",
            "Epoch 4/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 65.8431 - val_loss: 51.4161\n",
            "Epoch 5/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 50.2341 - val_loss: 38.7895\n",
            "Epoch 6/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 38.7293 - val_loss: 29.5631\n",
            "Epoch 7/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 30.0018 - val_loss: 22.5403\n",
            "Epoch 8/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 23.3831 - val_loss: 17.2328\n",
            "Epoch 9/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 18.3849 - val_loss: 13.2830\n",
            "Epoch 10/150\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 14.6340 - val_loss: 10.3743\n",
            "Epoch 11/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 11.7477 - val_loss: 8.1950\n",
            "Epoch 12/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 9.5764 - val_loss: 6.5037\n",
            "Epoch 13/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 7.8865 - val_loss: 5.2196\n",
            "Epoch 14/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 6.5822 - val_loss: 4.2476\n",
            "Epoch 15/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 5.5634 - val_loss: 3.5412\n",
            "Epoch 16/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 4.7436 - val_loss: 2.9281\n",
            "Epoch 17/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 4.0908 - val_loss: 2.4684\n",
            "Epoch 18/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 3.5605 - val_loss: 2.1184\n",
            "Epoch 19/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 3.1279 - val_loss: 1.8538\n",
            "Epoch 20/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 2.7552 - val_loss: 1.5760\n",
            "Epoch 21/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 2.4427 - val_loss: 1.3871\n",
            "Epoch 22/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 2.1834 - val_loss: 1.2298\n",
            "Epoch 23/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.9596 - val_loss: 1.1285\n",
            "Epoch 24/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.7669 - val_loss: 1.0084\n",
            "Epoch 25/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.5970 - val_loss: 0.8701\n",
            "Epoch 26/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.4459 - val_loss: 0.7872\n",
            "Epoch 27/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.3134 - val_loss: 0.7080\n",
            "Epoch 28/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.1959 - val_loss: 0.6556\n",
            "Epoch 29/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.0880 - val_loss: 0.5803\n",
            "Epoch 30/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.9979 - val_loss: 0.5555\n",
            "Epoch 31/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.9207 - val_loss: 0.4895\n",
            "Epoch 32/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.8465 - val_loss: 0.4519\n",
            "Epoch 33/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.7785 - val_loss: 0.4113\n",
            "Epoch 34/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.7231 - val_loss: 0.3967\n",
            "Epoch 35/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.6686 - val_loss: 0.3402\n",
            "Epoch 36/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.6155 - val_loss: 0.3195\n",
            "Epoch 37/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.5736 - val_loss: 0.2959\n",
            "Epoch 38/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.5311 - val_loss: 0.2847\n",
            "Epoch 39/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4954 - val_loss: 0.2565\n",
            "Epoch 40/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4621 - val_loss: 0.2333\n",
            "Epoch 41/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4286 - val_loss: 0.2188\n",
            "Epoch 42/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4007 - val_loss: 0.2143\n",
            "Epoch 43/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3760 - val_loss: 0.1964\n",
            "Epoch 44/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3492 - val_loss: 0.1714\n",
            "Epoch 45/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3323 - val_loss: 0.1601\n",
            "Epoch 46/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3088 - val_loss: 0.1523\n",
            "Epoch 47/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2915 - val_loss: 0.1448\n",
            "Epoch 48/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2706 - val_loss: 0.1287\n",
            "Epoch 49/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2540 - val_loss: 0.1252\n",
            "Epoch 50/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2377 - val_loss: 0.1170\n",
            "Epoch 51/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2280 - val_loss: 0.1089\n",
            "Epoch 52/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2117 - val_loss: 0.1019\n",
            "Epoch 53/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1987 - val_loss: 0.0958\n",
            "Epoch 54/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1924 - val_loss: 0.0966\n",
            "Epoch 55/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1772 - val_loss: 0.0916\n",
            "Epoch 56/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1689 - val_loss: 0.1132\n",
            "Epoch 57/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1611 - val_loss: 0.1034\n",
            "Epoch 58/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1537 - val_loss: 0.0936\n",
            "Epoch 59/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1481 - val_loss: 0.0700\n",
            "Epoch 60/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1356 - val_loss: 0.0683\n",
            "Epoch 61/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1304 - val_loss: 0.0598\n",
            "Epoch 62/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1222 - val_loss: 0.0585\n",
            "Epoch 63/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1175 - val_loss: 0.0602\n",
            "Epoch 64/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1099 - val_loss: 0.0621\n",
            "Epoch 65/150\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0483\n",
            "Epoch 66/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0991 - val_loss: 0.0480\n",
            "Epoch 67/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0941 - val_loss: 0.0478\n",
            "Epoch 68/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.0474\n",
            "Epoch 69/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.0409\n",
            "Epoch 70/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0826 - val_loss: 0.0399\n",
            "Epoch 71/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0749 - val_loss: 0.0446\n",
            "Epoch 72/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.0395\n",
            "Epoch 73/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.0338\n",
            "Epoch 74/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0677 - val_loss: 0.0488\n",
            "Epoch 75/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0633 - val_loss: 0.0318\n",
            "Epoch 76/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0612 - val_loss: 0.0338\n",
            "Epoch 77/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0282\n",
            "Epoch 78/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.0280\n",
            "Epoch 79/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0527 - val_loss: 0.0372\n",
            "Epoch 80/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0508 - val_loss: 0.0406\n",
            "Epoch 81/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0485 - val_loss: 0.0375\n",
            "Epoch 82/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0456 - val_loss: 0.0244\n",
            "Epoch 83/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0481 - val_loss: 0.0222\n",
            "Epoch 84/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0430 - val_loss: 0.0412\n",
            "Epoch 85/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0411 - val_loss: 0.0202\n",
            "Epoch 86/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0379 - val_loss: 0.0229\n",
            "Epoch 87/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0254\n",
            "Epoch 88/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0355 - val_loss: 0.0219\n",
            "Epoch 89/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.0191\n",
            "Epoch 90/150\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0312 - val_loss: 0.0186\n",
            "Epoch 91/150\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0325 - val_loss: 0.0166\n",
            "Epoch 92/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0305 - val_loss: 0.0166\n",
            "Epoch 93/150\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0279 - val_loss: 0.0153\n",
            "Epoch 94/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0286 - val_loss: 0.0169\n",
            "Epoch 95/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0167\n",
            "Epoch 96/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0250 - val_loss: 0.0149\n",
            "Epoch 97/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0243 - val_loss: 0.0152\n",
            "Epoch 98/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0246 - val_loss: 0.0139\n",
            "Epoch 99/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0243 - val_loss: 0.0217\n",
            "Epoch 100/150\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0227 - val_loss: 0.0153\n",
            "Epoch 101/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.0126\n",
            "Epoch 102/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.0139\n",
            "Epoch 103/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0212 - val_loss: 0.0132\n",
            "Epoch 104/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0211 - val_loss: 0.0136\n",
            "Epoch 105/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0192 - val_loss: 0.0252\n",
            "Epoch 106/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0231 - val_loss: 0.0131\n",
            "Epoch 107/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.0141\n",
            "Epoch 108/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0200 - val_loss: 0.0141\n",
            "Epoch 109/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.0164\n",
            "Epoch 110/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.0169\n",
            "Epoch 111/150\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0220\n",
            "Epoch 111: early stopping\n",
            "92/92 [==============================] - 0s 2ms/step\n",
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try three different step sizes**"
      ],
      "metadata": {
        "id": "b0r84uoJ8k9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = .5\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "RNN_step = model.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'RNN validation loss: {RNN_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NNPlHsu4m9f",
        "outputId": "66f5a6b2-9268-4257-87f4-8b21cf9837a5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 3.0790 - val_loss: 0.1678\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1100 - val_loss: 0.0720\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0792 - val_loss: 0.0600\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0614 - val_loss: 0.0461\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0394\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.0453 - val_loss: 0.0350\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0414 - val_loss: 0.0306\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0380 - val_loss: 0.0287\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0353 - val_loss: 0.0281\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0329 - val_loss: 0.0264\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0314 - val_loss: 0.0228\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0299 - val_loss: 0.0208\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0276 - val_loss: 0.0195\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0273 - val_loss: 0.0211\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0263 - val_loss: 0.0177\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0244 - val_loss: 0.0186\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0242 - val_loss: 0.0162\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0227 - val_loss: 0.0168\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0222 - val_loss: 0.0146\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0218 - val_loss: 0.0147\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.0206 - val_loss: 0.0136\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.0198 - val_loss: 0.0134\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.0199 - val_loss: 0.0127\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0185 - val_loss: 0.0130\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0183 - val_loss: 0.0120\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0176 - val_loss: 0.0115\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0170 - val_loss: 0.0124\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0168 - val_loss: 0.0113\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.0118\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0123\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0103\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0102\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0094\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0093\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0097\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0088\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0101\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0082\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0087\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0085\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0094\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0082\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0118 - val_loss: 0.0086\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0081\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0105\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0079\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0104\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0074\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0072\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0082\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0105 - val_loss: 0.0076\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0104 - val_loss: 0.0071\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0101 - val_loss: 0.0085\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0101 - val_loss: 0.0091\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0074\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0096 - val_loss: 0.0070\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0096 - val_loss: 0.0068\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0101 - val_loss: 0.0087\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0093 - val_loss: 0.0070\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.0103 - val_loss: 0.0080\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 1s 14ms/step - loss: 0.0095 - val_loss: 0.0068\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.0089 - val_loss: 0.0077\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.0091 - val_loss: 0.0072\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0094 - val_loss: 0.0097\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0086 - val_loss: 0.0077\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0093 - val_loss: 0.0069\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0086 - val_loss: 0.0065\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0090 - val_loss: 0.0076\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0092 - val_loss: 0.0082\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0093 - val_loss: 0.0070\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.0082 - val_loss: 0.0068\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0083 - val_loss: 0.0071\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.0065\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0083\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0083 - val_loss: 0.0082\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0086 - val_loss: 0.0067\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0078 - val_loss: 0.0091\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0090 - val_loss: 0.0065\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.0084 - val_loss: 0.0067\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.0097 - val_loss: 0.0068\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.0083 - val_loss: 0.0066\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0079 - val_loss: 0.0073\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.0071\n",
            "Epoch 83: early stopping\n",
            "RNN validation loss: 0.0071038552559912205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 5\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "RNN_step = model.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'RNN validation loss: {RNN_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyMSrlA07Xhp",
        "outputId": "9ce123a5-4c6f-4a96-9b54-4a928d6efafa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 42.0484 - val_loss: 8.2103\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 7.6097 - val_loss: 3.9036\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 4.5077 - val_loss: 2.6729\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 3.1252 - val_loss: 1.8803\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 2.3974 - val_loss: 1.4439\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.9085 - val_loss: 1.2228\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 1.5749 - val_loss: 1.0210\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.3405 - val_loss: 0.9175\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.1376 - val_loss: 0.7972\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.9899 - val_loss: 0.6744\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.8864 - val_loss: 0.6262\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.7783 - val_loss: 0.5220\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6861 - val_loss: 0.5432\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.6264 - val_loss: 0.4412\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5618 - val_loss: 0.4017\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5152 - val_loss: 0.3921\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4809 - val_loss: 0.3712\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4555 - val_loss: 0.3505\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4105 - val_loss: 0.3120\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3834 - val_loss: 0.2835\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3671 - val_loss: 0.2646\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3351 - val_loss: 0.2190\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3069 - val_loss: 0.2300\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2939 - val_loss: 0.2189\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2824 - val_loss: 0.2131\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2843 - val_loss: 0.1965\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2492 - val_loss: 0.1573\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.2262 - val_loss: 0.1761\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2127 - val_loss: 0.1528\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1969 - val_loss: 0.1435\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1940 - val_loss: 0.1309\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1813 - val_loss: 0.1350\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1807 - val_loss: 0.1288\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1669 - val_loss: 0.1335\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1622 - val_loss: 0.1196\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1465 - val_loss: 0.1401\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1458 - val_loss: 0.1433\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1457 - val_loss: 0.1422\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1390 - val_loss: 0.1358\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1356 - val_loss: 0.1739\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1361 - val_loss: 0.1075\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1213 - val_loss: 0.1310\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1198 - val_loss: 0.1039\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 0.1083\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1113 - val_loss: 0.1007\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.1091 - val_loss: 0.1030\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0950 - val_loss: 0.1086\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1050 - val_loss: 0.1192\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1021 - val_loss: 0.1056\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0950 - val_loss: 0.1015\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0890 - val_loss: 0.0881\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0813 - val_loss: 0.0996\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0807 - val_loss: 0.1077\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0782 - val_loss: 0.1013\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0700\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0798\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0816\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0583 - val_loss: 0.0778\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0608 - val_loss: 0.0556\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0633 - val_loss: 0.0624\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0590 - val_loss: 0.0536\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0491 - val_loss: 0.0644\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0484 - val_loss: 0.0509\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0776\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0784 - val_loss: 0.0943\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0581\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0701 - val_loss: 0.0516\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0437 - val_loss: 0.0512\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0418 - val_loss: 0.0463\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0386 - val_loss: 0.0870\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0433 - val_loss: 0.0557\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0522 - val_loss: 0.0470\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0422 - val_loss: 0.0431\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0352 - val_loss: 0.0400\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.0380\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0395 - val_loss: 0.0403\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0396\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.1419\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0402 - val_loss: 0.0401\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1001\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.0416\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.0417\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0452\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0311 - val_loss: 0.0383\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0306 - val_loss: 0.0495\n",
            "Epoch 85: early stopping\n",
            "RNN validation loss: 0.04951838031411171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 20\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "RNN_step = model.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'RNN validation loss: {RNN_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_ITQ75Y7ZHJ",
        "outputId": "fe1b0317-48d8-43e6-d149-b199f2acef49"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 3.0994 - val_loss: 1.3968\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.6287 - val_loss: 1.1844\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.4331 - val_loss: 0.9192\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 1.1495 - val_loss: 0.7800\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.9506 - val_loss: 0.6362\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.8397 - val_loss: 0.6020\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.8640 - val_loss: 0.5183\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.8726 - val_loss: 0.5217\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6978 - val_loss: 0.5058\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.7554 - val_loss: 0.5113\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6986 - val_loss: 0.7053\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6452 - val_loss: 0.4391\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6805 - val_loss: 0.3692\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5752 - val_loss: 0.5939\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5966 - val_loss: 0.6491\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5422 - val_loss: 0.4693\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4806 - val_loss: 0.4401\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4434 - val_loss: 0.3885\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4642 - val_loss: 0.5654\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4287 - val_loss: 0.4108\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4739 - val_loss: 0.3740\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.4027 - val_loss: 0.4009\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4026 - val_loss: 0.4325\n",
            "Epoch 23: early stopping\n",
            "RNN validation loss: 0.4324830174446106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU Model**"
      ],
      "metadata": {
        "id": "uiq7wam77eHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bulding a model with GRU\n",
        "model_1 = Sequential()\n",
        "model_1.add(GRU(units=32, input_shape=(1,step),activation=\"tanh\"))\n",
        "model_1.add(Dense(8,activation= \"relu\"))\n",
        "model_1.add(Dense(1))\n",
        "model_1.compile(loss='mse', optimizer= 'adam')\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yJ2ed96VOjm",
        "outputId": "d89e2414-27cc-4e55-d0df-dbbd394bded7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_1 (GRU)                 (None, 32)                3744      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 264       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,017\n",
            "Trainable params: 4,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model with train data and predict validation data\n",
        "model_1.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=100,batch_size=32,callbacks=[earlyStopping])\n",
        "trainPredict = model_1.predict(X_train)\n",
        "validPredict=model_1.predict(X_valid)\n",
        "predicted=np.concatenate((trainPredict,validPredict),axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoyOrR9Sg0mo",
        "outputId": "0972394d-8aab-4847-c801-6f43b2af8e91"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 3s 9ms/step - loss: 255.3790 - val_loss: 129.1819\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 53.9804 - val_loss: 17.1380\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 11.9958 - val_loss: 4.7636\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 4.0944 - val_loss: 1.9247\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 2.2009 - val_loss: 1.2808\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.5179 - val_loss: 0.9038\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.1085 - val_loss: 0.6480\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.8175 - val_loss: 0.4440\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6208 - val_loss: 0.3265\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4799 - val_loss: 0.2449\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3741 - val_loss: 0.1921\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2938 - val_loss: 0.1602\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2477 - val_loss: 0.1704\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2099 - val_loss: 0.1106\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1746 - val_loss: 0.1141\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1518 - val_loss: 0.0793\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.1311 - val_loss: 0.0795\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.1152 - val_loss: 0.0663\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1044 - val_loss: 0.0554\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0916 - val_loss: 0.0523\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.0684\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0872 - val_loss: 0.0446\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0668 - val_loss: 0.0388\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0663 - val_loss: 0.0347\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.0324\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0476\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0567 - val_loss: 0.0350\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0464 - val_loss: 0.0365\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0280\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0437 - val_loss: 0.0249\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0414 - val_loss: 0.0277\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.0241\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0362 - val_loss: 0.0294\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.0190\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0317 - val_loss: 0.0243\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.0317\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.0214\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0262\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0424\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0282 - val_loss: 0.0159\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0239 - val_loss: 0.0213\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0239 - val_loss: 0.0167\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0246 - val_loss: 0.0189\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0136\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0205 - val_loss: 0.0145\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0205 - val_loss: 0.0124\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0207 - val_loss: 0.0144\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0231 - val_loss: 0.0220\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0254 - val_loss: 0.0213\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.0122\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0114\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0113\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.0118\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0110\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0162 - val_loss: 0.0097\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0187 - val_loss: 0.0122\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0142 - val_loss: 0.0114\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0148 - val_loss: 0.0102\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0144 - val_loss: 0.0091\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0169 - val_loss: 0.0085\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0120\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0112\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0160\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0197 - val_loss: 0.0223\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0077\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0082\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0089\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0097\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0265\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0099\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0119 - val_loss: 0.0185\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0115\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0214 - val_loss: 0.0083\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0072\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0176\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0181\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0100\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0076\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0072\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0091\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0198\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0111 - val_loss: 0.0097\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0075\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 1s 13ms/step - loss: 0.0104 - val_loss: 0.0131\n",
            "Epoch 84: early stopping\n",
            "92/92 [==============================] - 1s 3ms/step\n",
            "23/23 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try three different step sizes**"
      ],
      "metadata": {
        "id": "Wmj9VBVD8y90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = .5\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "GRU_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'GRU validation loss: {GRU_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHHsZ9RQ6bIq",
        "outputId": "d242e024-9e6e-4a1b-83a3-92b65f94575b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 24.9522 - val_loss: 4.0620\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 3.3939 - val_loss: 2.4660\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 2.4242 - val_loss: 2.2322\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 2.1486 - val_loss: 1.4486\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.7619 - val_loss: 1.2127\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.4592 - val_loss: 1.4408\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 1.4027 - val_loss: 1.3669\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 1.2704 - val_loss: 0.9510\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 1.1050 - val_loss: 0.9380\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 1.0161 - val_loss: 0.8513\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.8795 - val_loss: 0.8400\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.8383 - val_loss: 0.7330\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.7685 - val_loss: 0.7335\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.6712 - val_loss: 1.1245\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6559 - val_loss: 0.5941\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6554 - val_loss: 0.5888\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.6385 - val_loss: 0.7040\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5859 - val_loss: 0.8197\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5460 - val_loss: 0.6449\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5567 - val_loss: 0.5771\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.5236 - val_loss: 0.5158\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.5050 - val_loss: 0.4594\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4499 - val_loss: 0.3597\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.4850 - val_loss: 0.4025\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.4437 - val_loss: 0.4612\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.4225 - val_loss: 0.3518\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3854 - val_loss: 0.3874\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3705 - val_loss: 0.2854\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3972 - val_loss: 0.3277\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3453 - val_loss: 0.3463\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3964 - val_loss: 0.9009\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4033 - val_loss: 0.2634\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.3463 - val_loss: 0.2925\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2903 - val_loss: 0.2658\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3034 - val_loss: 0.2755\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.3001 - val_loss: 0.2406\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3063 - val_loss: 0.4659\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3061 - val_loss: 0.2364\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2474 - val_loss: 0.1778\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3647 - val_loss: 0.3022\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3052 - val_loss: 0.2019\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2119 - val_loss: 0.2610\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2519 - val_loss: 0.1557\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2096 - val_loss: 0.1352\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1709 - val_loss: 0.1257\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2241 - val_loss: 0.1527\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2176 - val_loss: 0.4234\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2240 - val_loss: 0.1360\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1745 - val_loss: 0.1069\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2222 - val_loss: 0.2666\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2750 - val_loss: 0.1282\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1803 - val_loss: 0.1380\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1522 - val_loss: 0.1673\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2072 - val_loss: 0.3305\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.3135 - val_loss: 0.2455\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.1958 - val_loss: 0.1802\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1533 - val_loss: 0.0875\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.1212 - val_loss: 0.1672\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1043 - val_loss: 0.0991\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.1393\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1051 - val_loss: 0.1234\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1655 - val_loss: 0.0911\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1244 - val_loss: 0.1058\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0937 - val_loss: 0.1181\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1459 - val_loss: 0.1450\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2234 - val_loss: 0.0808\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1209 - val_loss: 0.2556\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2137 - val_loss: 0.1558\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1303 - val_loss: 0.1314\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1079 - val_loss: 0.1335\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0995 - val_loss: 0.1516\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1407 - val_loss: 0.0888\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2220 - val_loss: 0.1057\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1131 - val_loss: 0.0639\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0954\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1309 - val_loss: 0.1647\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1568 - val_loss: 0.0774\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1339 - val_loss: 0.0990\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2099 - val_loss: 0.1589\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.2162 - val_loss: 0.1068\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1540 - val_loss: 0.0917\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1317 - val_loss: 0.0914\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.2115 - val_loss: 0.1070\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0757 - val_loss: 0.1476\n",
            "Epoch 84: early stopping\n",
            "GRU validation loss: 0.14759932458400726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 5\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "GRU_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'GRU validation loss: {GRU_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TiUbbm7oMf",
        "outputId": "fbfce448-5ec2-450e-9126-e6b1c65c52f4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1221 - val_loss: 0.1055\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1593 - val_loss: 0.0785\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0748 - val_loss: 0.0825\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0865 - val_loss: 0.1211\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0915 - val_loss: 0.0753\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1106 - val_loss: 0.0821\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0686\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0980 - val_loss: 0.0917\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.0973\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1047 - val_loss: 0.1295\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1833 - val_loss: 0.1414\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1627 - val_loss: 0.1403\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1468 - val_loss: 0.1138\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.0936\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.1980\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0619\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1123 - val_loss: 0.1060\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1380 - val_loss: 0.0600\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1212 - val_loss: 0.1358\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.0725\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3726 - val_loss: 0.2077\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1552 - val_loss: 0.1323\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.3083 - val_loss: 0.4043\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1638 - val_loss: 0.0690\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0745 - val_loss: 0.1092\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1447 - val_loss: 0.1026\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0846 - val_loss: 0.1105\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.1278\n",
            "Epoch 28: early stopping\n",
            "GRU validation loss: 0.12780700623989105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 20\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "GRU_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'GRU validation loss: {GRU_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucuP5H_67pzk",
        "outputId": "4f007e09-e94d-4562-806f-ec7ac9b27e62"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1284 - val_loss: 0.1259\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1168 - val_loss: 0.0682\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0843 - val_loss: 0.1678\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1025 - val_loss: 0.0521\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0965\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1323 - val_loss: 0.1114\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0923 - val_loss: 0.0879\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0973\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1367 - val_loss: 0.0776\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1692 - val_loss: 0.0815\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1049 - val_loss: 0.1052\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1040 - val_loss: 0.0649\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0813 - val_loss: 0.0504\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0989\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0729 - val_loss: 0.1548\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0828 - val_loss: 0.0932\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0944 - val_loss: 0.1361\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1629 - val_loss: 0.0790\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1611 - val_loss: 0.0845\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0552 - val_loss: 0.0638\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0805 - val_loss: 0.1098\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1915 - val_loss: 0.0731\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1287 - val_loss: 0.0905\n",
            "Epoch 23: early stopping\n",
            "GRU validation loss: 0.09054933488368988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM Model**"
      ],
      "metadata": {
        "id": "y7fFbxxR7sQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bulding a model with LSTM\n",
        "model_2 = Sequential()\n",
        "model_2.add(LSTM(units=32, input_shape=(1,step),activation=\"tanh\"))\n",
        "model_2.add(Dense(8,activation= \"relu\"))\n",
        "model_2.add(Dense(1))\n",
        "model_2.compile(loss='mse', optimizer= 'adam')\n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6KcqZ-WVtVo",
        "outputId": "e199d0bd-323d-478a-d175-e849c830720d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 32)                4864      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 8)                 264       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,137\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model with train data and predict validation data\n",
        "model_2.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=100,batch_size=32,callbacks=[earlyStopping])\n",
        "trainPredict = model_2.predict(X_train)\n",
        "validPredict=model_2.predict(X_valid)\n",
        "predicted=np.concatenate((trainPredict,validPredict),axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeIWhdgNhJ4f",
        "outputId": "6276d6ad-5342-43e1-bc86-914ce15cbed3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 3s 10ms/step - loss: 278.8158 - val_loss: 180.5876\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 92.5898 - val_loss: 32.3007\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 25.7448 - val_loss: 14.5963\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 12.4418 - val_loss: 6.4261\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 6.1334 - val_loss: 3.2955\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 3.7854 - val_loss: 2.2597\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 2.7214 - val_loss: 1.6743\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 2.0465 - val_loss: 1.2446\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 1.5669 - val_loss: 0.9524\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 1.1969 - val_loss: 0.7481\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.9348 - val_loss: 0.5870\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.7340 - val_loss: 0.3895\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.5622 - val_loss: 0.3045\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.4582 - val_loss: 0.4351\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 1s 12ms/step - loss: 0.4037 - val_loss: 0.2035\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.3185 - val_loss: 0.1777\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 1s 10ms/step - loss: 0.2746 - val_loss: 0.1415\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.2385 - val_loss: 0.2161\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 1s 11ms/step - loss: 0.2097 - val_loss: 0.1157\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.1859 - val_loss: 0.0974\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1663 - val_loss: 0.0890\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1456 - val_loss: 0.0872\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1242\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1183 - val_loss: 0.1174\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1125 - val_loss: 0.0621\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0983 - val_loss: 0.0562\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1049 - val_loss: 0.0509\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0849 - val_loss: 0.0883\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0875 - val_loss: 0.0427\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0387\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0761 - val_loss: 0.0396\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0628 - val_loss: 0.0458\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0590 - val_loss: 0.0569\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0656 - val_loss: 0.0343\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0507 - val_loss: 0.0290\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0493 - val_loss: 0.0270\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0497 - val_loss: 0.0352\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0490 - val_loss: 0.0676\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0435 - val_loss: 0.0327\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0381 - val_loss: 0.0216\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0372 - val_loss: 0.0205\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0209\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0334 - val_loss: 0.0189\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0337 - val_loss: 0.0456\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0305 - val_loss: 0.0265\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0173\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0181\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0276 - val_loss: 0.0185\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0261 - val_loss: 0.0205\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0266 - val_loss: 0.0184\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0227 - val_loss: 0.0153\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0137\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0204\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.0155\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0196 - val_loss: 0.0123\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0191 - val_loss: 0.0219\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0203 - val_loss: 0.0162\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0289 - val_loss: 0.0194\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.0133\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0315\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0107\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.0132\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0129\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0184\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0258 - val_loss: 0.0121\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0125\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0129\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0139\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0293\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0143 - val_loss: 0.0186\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0138 - val_loss: 0.0096\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0158\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0144 - val_loss: 0.0094\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0122 - val_loss: 0.0109\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0180 - val_loss: 0.0283\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0165 - val_loss: 0.0129\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 1s 9ms/step - loss: 0.0135 - val_loss: 0.0133\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0137 - val_loss: 0.0102\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0127\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0088\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0126\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0085\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0124\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0092\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0133 - val_loss: 0.0110\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0088\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0101\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0082\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0108\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0262\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0079\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0151 - val_loss: 0.0094\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.0082\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0102\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0117\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0094\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0181\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0163 - val_loss: 0.0218\n",
            "92/92 [==============================] - 1s 3ms/step\n",
            "23/23 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try three different step sizes**"
      ],
      "metadata": {
        "id": "POYrVCxE8-o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = .5\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "LSTM_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'LSTM validation loss: {LSTM_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxKvNFC97vHS",
        "outputId": "887b0e4b-b7b7-42b8-c800-5f19de5565e2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0660 - val_loss: 0.1256\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0619 - val_loss: 0.0430\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0558 - val_loss: 0.0540\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0667 - val_loss: 0.0536\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0798 - val_loss: 0.0674\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0521 - val_loss: 0.0957\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0442\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0654 - val_loss: 0.0444\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0377 - val_loss: 0.0576\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0596 - val_loss: 0.0689\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0583 - val_loss: 0.0385\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0418 - val_loss: 0.0822\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0421\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1020 - val_loss: 0.0557\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0464 - val_loss: 0.0563\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0525 - val_loss: 0.0671\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0508 - val_loss: 0.0643\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0393 - val_loss: 0.0391\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0452 - val_loss: 0.0962\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0895 - val_loss: 0.0969\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.1928 - val_loss: 0.0695\n",
            "Epoch 21: early stopping\n",
            "LSTM validation loss: 0.0694691464304924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 5\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "LSTM_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'LSTM validation loss: {LSTM_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQs6xVHK71Is",
        "outputId": "41c42705-7dda-4632-9e3d-7228ff0d4640"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0817 - val_loss: 0.0663\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0495 - val_loss: 0.1739\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0628 - val_loss: 0.0329\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0412 - val_loss: 0.0409\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0470\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0756 - val_loss: 0.0653\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0961 - val_loss: 0.0386\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0749 - val_loss: 0.0439\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0563 - val_loss: 0.0417\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0542 - val_loss: 0.1062\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0617 - val_loss: 0.1106\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0675 - val_loss: 0.0683\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0563 - val_loss: 0.0438\n",
            "Epoch 13: early stopping\n",
            "LSTM validation loss: 0.04381118342280388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_sizes = 20\n",
        "X_train_step = np.cumsum(X_train, axis=1) * step_size\n",
        "X_val_step = np.cumsum(X_valid, axis=1) * step_size\n",
        "\n",
        "LSTM_step = model_1.fit(X_train_step, y_train, validation_data=(X_val_step, y_valid), epochs=100, batch_size=32, callbacks=[earlyStopping])\n",
        "print(f'LSTM validation loss: {LSTM_step.history[\"val_loss\"][-1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ6YFGRf73HN",
        "outputId": "c15cd313-1c1f-47ce-95f0-6a2559b8324e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0509 - val_loss: 0.0757\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0749 - val_loss: 0.0633\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0443 - val_loss: 0.0323\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0714 - val_loss: 0.0812\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0728 - val_loss: 0.0627\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0931 - val_loss: 0.0931\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0713 - val_loss: 0.0537\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0559 - val_loss: 0.0453\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 1s 5ms/step - loss: 0.0368 - val_loss: 0.0259\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0750 - val_loss: 0.1191\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.0613 - val_loss: 0.0306\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0633 - val_loss: 0.0526\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 0s 5ms/step - loss: 0.0948 - val_loss: 0.0831\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.0815\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0473 - val_loss: 0.0741\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0497 - val_loss: 0.0632\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0670 - val_loss: 0.0524\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.0535 - val_loss: 0.0803\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0690\n",
            "Epoch 19: early stopping\n",
            "LSTM validation loss: 0.06900686770677567\n"
          ]
        }
      ]
    }
  ]
}